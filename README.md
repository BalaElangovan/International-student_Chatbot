# International-student_Chatbot

# 1. Chatbot Code Explained
## Technologies and Libraries Used:

Dash Framework: I've utilized the Dash framework for building the web interface of the chatbot. This allows for creating a responsive and interactive user experience.

Natural Language Processing: The spaCy library is employed for processing user inputs. This enables the chatbot to understand and analyze natural language effectively.

Machine Learning Integration: TensorFlow's Keras library is included, indicating the use of machine learning models, although the specific application is not detailed in the analyzed portion of the script.

Data Handling and Regular Expressions: I've used JSON, NumPy, and Pickle for data management, alongside regular expressions for text processing tasks.

![banner](https://github.com/BalaElangovan/International-student_Chatbot/blob/main/chatbot.png)

## Core Functionalities:

Processing User Inputs: The chatbot normalizes and tokenizes user inputs, preparing them for analysis and response generation.

Country Recognition: A specific function, country_in_query, is designed to detect country names in user queries, which could be used for generating tailored responses.

Keyword-Based Responses: The script includes a method, get_keyword_response, which matches user input with predefined responses based on keyword analysis.

Text Processing: The use of spacy and regular expressions suggests that the script processes user inputs to understand and respond to them. This is typical in chatbots for understanding context or intent.

Interactive Web Interface: The usage of Dash components indicates that the script provides an interactive interface, where users can type in their queries, and the chatbot responds.

## Operation:

The chatbot operates by receiving queries from users, processing these inputs to understand their context or intent, and then generating appropriate responses.

The use of tokenization, entity recognition (like countries), and a scoring mechanism for selecting responses, demonstrates an efficient method of handling and responding to user queries.

The web-based interface, powered by Dash, facilitates real-time interaction with users.

# 2. Train Code Explained

## Technologies and Libraries:
I have employed TensorFlow with its Keras API, NumPy for numerical operations, and Scikit-Learnâ€™s LabelEncoder for data encoding. The use of a Tokenizer and pad_sequences from Keras is crucial for text data preprocessing, and the inclusion of a Dropout layer signifies my approach to mitigate overfitting.

## Core Functionalities:

Loading Data: The script loads data from a JSON file, specifically 'intents.json'. This file is integral as it contains the training data necessary for the model.

Model Building and Training: I have used a Sequential model with Dense layers and an Embedding layer, along with GlobalAveragePooling1D. These elements are essential in handling and processing text data.
The Embedding layer is particularly important for transforming words into vector representations, while Dense layers construct a fully connected neural network, suitable for either classification or regression tasks.
Working of the Script

## Data Preparation:

The script pre-processes the text data, tokenizes it, and encodes labels. This step is vital to convert textual data into a machine-readable format for effective training.

## Training Process:

I build and train a neural network model on this processed data. The Dropout layer here plays a key role in optimizing the model by preventing overfitting, ensuring a robust and generalizable model.

## Model Output:

Post-training, this model is capable of predicting or classifying new inputs. The learning it gains from the 'intents.json' data empowers it to effectively respond to similar data inputs in the future.

# 3. Intents Dataset file Explained
## Dataset Overview:

The 'intents.json' file is structured as a collection of intents. Each intent is associated with a list of patterns (user inputs) and responses.
Each entry in the dataset is categorized under a 'tag', which represents the intent or topic of the conversation.
The 'patterns' are typical user queries or statements related to the specific intent.
The 'responses' are the predefined replies that the chatbot should give when it recognizes a pattern matching the intent.

## Key Features of the Dataset:
Diverse Intents: The dataset covers a range of topics or intents, such as travel information, accommodation help, and academic guidance. This diversity is essential for training a chatbot that can handle various user queries.

Pattern-Response Pairs: Each intent includes multiple pattern-response pairs. This variety in patterns helps in training the model to recognize different ways users might express a similar intent.

Real-World Applications: The intents are reflective of real-world scenarios, such as finding accommodations, understanding travel systems, or applying for academic courses. This practical focus ensures the chatbot is trained on realistic and relevant data.
